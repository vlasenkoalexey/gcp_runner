{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ai_platform_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    echo \"Submitting an AI Platform job...\"\n",
    "    # see https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "    gcloud beta ai-platform jobs submit training ${JOB_NAME} \\\n",
    "            ${CONFIG_FIX} \\\n",
    "            --scale-tier=${SCALE_TIER} \\\n",
    "            --region=${REGION} \\\n",
    "            --master-image-uri=${IMAGE_URI} \\\n",
    "            --stream-logs \\\n",
    "            ${CONFIG} \\\n",
    "            -- python trainer/trainer.py --job-dir=${MODEL_DIR} --train-location=cloud $@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_ai_platform_constants.ipynb.\n",
      "Converted ai_platform_constants.ipynb.\n",
      "Converted ai_platform_runner.ipynb.\n",
      "Converted entry_point.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted kubernetes_runner.ipynb.\n",
      "Converted local_runner.ipynb.\n",
      "Converted sample_code_test.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from gcp_runner import core\n",
    "core.export_and_reload_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import inspect \n",
    "from enum import Enum\n",
    "from gcp_runner.ai_platform_constants import * \n",
    "\n",
    "def _get_not_none(arg, default):\n",
    "    return default if arg is None else arg\n",
    "\n",
    "def _format_arg(key, value):\n",
    "    return \"--%s=%s\" % (key.replace('_', '-'), value)\n",
    "\n",
    "def _append_arg(args, key, value):\n",
    "    if value is not None:\n",
    "        if issubclass(type(value), Enum):\n",
    "            value = value.value\n",
    "        if issubclass(type(value), int) and value == 0:\n",
    "            return\n",
    "            \n",
    "        args.append(_format_arg(key, value))\n",
    "\n",
    "def _get_common_args(\n",
    "    job_dir,\n",
    "    job_name=None,\n",
    "    region=None,\n",
    "    scale_tier: ScaleTier = ScaleTier.CUSTOM,\n",
    "    master_machine_type: MachineType = None,\n",
    "    master_image_uri=None,\n",
    "    master_accelerator_type: AcceleratorType = None,\n",
    "    master_accelerator_count=None,\n",
    "    parameter_machine_type: MachineType = None,\n",
    "    parameter_machine_count=None,\n",
    "    parameter_image_uri=None,\n",
    "    parameter_accelerator_type: AcceleratorType = None,\n",
    "    parameter_accelerator_count=None,\n",
    "    worker_machine_type: MachineType = None,\n",
    "    worker_machine_count=None,\n",
    "    worker_image_uri=None,\n",
    "    worker_accelerator_type: AcceleratorType = None,\n",
    "    worker_accelerator_count=None,\n",
    "    use_chief_in_tf_config=True,\n",
    "    distribution_strategy_type: DistributionStrategyType = None):   \n",
    "    args = []\n",
    "    \n",
    "    if distribution_strategy_type and scale_tier == ScaleTier.CUSTOM:\n",
    "        master_machine_type = _get_not_none(master_machine_type, MachineType.N1_STANDARD_4)\n",
    "        if distribution_strategy_type == DistributionStrategyType.MIRRORED_STRATEGY:\n",
    "            master_accelerator_type = _get_not_none (master_accelerator_type, AcceleratorType.NVIDIA_TESLA_K80)\n",
    "            master_accelerator_count = _get_not_none(master_accelerator_count, 2)\n",
    "            worker_machine_count = _get_not_none(worker_machine_count, 0)\n",
    "        elif distribution_strategy_type == DistributionStrategyType.CENTRAL_STORAGE_STRATEGY:\n",
    "            master_accelerator_type = _get_not_none (master_accelerator_type, AcceleratorType.NVIDIA_TESLA_K80)\n",
    "            master_accelerator_count = _get_not_none(master_accelerator_count, 2)\n",
    "            worker_machine_count = _get_not_none(worker_machine_count, 0)\n",
    "        elif distribution_strategy_type == DistributionStrategyType.PARAMETER_SERVERSTRATEGY:\n",
    "            parameter_machine_type = _get_not_none(parameter_machine_type, MachineType.N1_STANDARD_4)\n",
    "            parameter_machine_count = _get_not_none(parameter_machine_count, 1)\n",
    "            worker_machine_count = _get_not_none(worker_machine_count, 2)\n",
    "            worker_machine_type = _get_not_none(worker_machine_type, MachineType.N1_STANDARD_4)\n",
    "        elif distribution_strategy_type == DistributionStrategyType.MULTI_WORKER_MIRRORED_STRATEGY:\n",
    "            master_accelerator_type = _get_not_none (master_accelerator_type, AcceleratorType.NVIDIA_TESLA_K80)\n",
    "            master_accelerator_count = _get_not_none(master_accelerator_count, 2)\n",
    "            worker_machine_count = _get_not_none(worker_machine_count, 2)\n",
    "            worker_accelerator_type = _get_not_none (worker_accelerator_type, AcceleratorType.NVIDIA_TESLA_K80)\n",
    "            worker_machine_type = _get_not_none(worker_machine_type, MachineType.N1_STANDARD_4)\n",
    "            \n",
    "    if distribution_strategy_type == DistributionStrategyType.TPU_STRATEGY and \\\n",
    "        (master_accelerator_count == 0 or master_accelerator_type is None):\n",
    "        print('changing scale_tier to BASIC_TPU to run with TPU_STRATEGY')\n",
    "        scale_tier = ScaleTier.BASIC_TPU\n",
    "\n",
    "# Nice way ot automatically setting flags, keeping for now for future reference        \n",
    "#     for key, value in locals().items():\n",
    "#         # print(key + '=' + str(value))\n",
    "#         if key != 'args' and key != 'distribution_strategy_type' and value:\n",
    "#             if key.endswith('_type') or key == 'scale_tier':\n",
    "#                 args.append(\"--%s=%s\" % (key.replace('_', '-'), value.value))\n",
    "#             else:\n",
    "#                 args.append(\"--%s=%s\" % (key.replace('_', '-'), value))\n",
    "\n",
    "    # For flags overview refer to \n",
    "    # https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "    accelerator_format = 'count=%d,type=%s'\n",
    "    _append_arg(args, 'job-dir', job_dir)\n",
    "    _append_arg(args, 'job-name', job_name)\n",
    "    _append_arg(args, 'region', region)\n",
    "    _append_arg(args, 'scale-tier', scale_tier)\n",
    "    \n",
    "    _append_arg(args, 'master-machine-type', master_machine_type)\n",
    "    _append_arg(args, 'master-image-uri', master_image_uri)\n",
    "    if master_accelerator_count is not None and master_accelerator_count > 0:\n",
    "        _append_arg(args, 'master-accelerator', 'count=%d,type=%s' % (master_accelerator_count, master_accelerator_type.value))\n",
    "\n",
    "    _append_arg(args, 'parameter-server-machine-type', parameter_machine_type)\n",
    "    _append_arg(args, 'parameter-server-count', parameter_machine_count)\n",
    "    _append_arg(args, 'parameter-server-image-uri', parameter_image_uri)\n",
    "    if parameter_accelerator_count is not None and parameter_accelerator_count > 0:\n",
    "        _append_arg(args, 'parameter-server-accelerator', 'count=%d,type=%s' % (parameter_accelerator_count, parameter_accelerator_type.value))\n",
    "        \n",
    "    _append_arg(args, 'worker-machine-type', worker_machine_type)\n",
    "    _append_arg(args, 'worker-count', worker_machine_count)\n",
    "    _append_arg(args, 'worker-image-uri', worker_image_uri)\n",
    "    if worker_accelerator_count is not None and worker_accelerator_count > 0:\n",
    "        _append_arg(args, 'worker-accelerator', 'count=%d,type=%s' % (worker_accelerator_count, worker_accelerator_type.value))\n",
    "\n",
    "    _append_arg(args, 'use-chief-in-tf-config', use_chief_in_tf_config)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--job-dir=job-dir/model',\n",
       " '--scale-tier=custom',\n",
       " '--master-machine-type=n1-standard-4',\n",
       " '--parameter-server-machine-type=n1-standard-4',\n",
       " '--parameter-server-count=1',\n",
       " '--worker-machine-type=n1-standard-4',\n",
       " '--worker-count=2',\n",
       " '--use-chief-in-tf-config=True']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_common_args('job-dir/model', distribution_strategy_type=DistributionStrategyType.PARAMETER_SERVERSTRATEGY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_ai_platform_run_args(job_dir, distribution_strategy_type, use_distribution_strategy_scope):\n",
    "    args.append(\"--job-dir=%s\" % job_dir)\n",
    "    if distribution_strategy_type:\n",
    "        args.append(\"--distribution-strategy-type=%s\" % distribution_strategy_type)\n",
    "        if use_distribution_strategy_scope:\n",
    "            args.append(\"--use-distribution-strategy-scope\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import datetime\n",
    "from gcp_runner.core import build_and_push_docker_image, run_process, get_run_python_args, format_job_dir, print_tensorboard_command\n",
    "from gcp_runner import ai_platform_constants\n",
    "\n",
    "def run_docker_image(\n",
    "    func, \n",
    "    job_dir,\n",
    "    build_docker_file=None,\n",
    "    dry_run=False,\n",
    "    job_name=None,\n",
    "    region=None,\n",
    "    scale_tier: ai_platform_constants.ScaleTier =  ai_platform_constants.ScaleTier.CUSTOM,\n",
    "    master_machine_type: ai_platform_constants.MachineType = None,\n",
    "    master_image_uri=None,\n",
    "    master_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    master_accelerator_count=None,\n",
    "    parameter_machine_type: ai_platform_constants.MachineType = None,\n",
    "    parameter_machine_count=None,\n",
    "    parameter_image_uri=None,\n",
    "    parameter_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    parameter_accelerator_count=None,\n",
    "    worker_machine_type: ai_platform_constants.MachineType = None,\n",
    "    worker_machine_count=None,\n",
    "    worker_image_uri=None,\n",
    "    worker_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    worker_accelerator_count=None,\n",
    "    use_chief_in_tf_config=True,\n",
    "    distribution_strategy_type: ai_platform_constants.DistributionStrategyType = None,\n",
    "    use_distribution_strategy_scope: bool = None,\n",
    "    **kwargs):\n",
    "    \n",
    "    if not master_image_uri:\n",
    "        raise ValueError('master_image_uri argument is required')\n",
    "    \n",
    "    date_time = datetime.datetime.now()\n",
    "    if not job_name:\n",
    "        job_name = 'ai_platform_runner_train_docker_' + date_time.strftime('%Y%m%d_%H%M%S')\n",
    "    job_dir = format_job_dir(job_dir, date_time=date_time)\n",
    "    print_tensorboard_command(job_dir)\n",
    "        \n",
    "    if build_docker_file is not None:\n",
    "        result = build_and_push_docker_image(build_docker_file, master_image_uri, dry_run=dry_run)\n",
    "        if result:\n",
    "            return result\n",
    "    \n",
    "    # see https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "    args = ['gcloud', 'ai-platform', 'jobs', 'submit', 'training', job_name,\n",
    "           \"--stream-logs\"]\n",
    "    \n",
    "    common_args = _get_common_args(\n",
    "        job_dir,\n",
    "        region = region,\n",
    "        scale_tier = scale_tier,\n",
    "        master_machine_type = master_machine_type,\n",
    "        master_image_uri = master_image_uri,\n",
    "        master_accelerator_type = master_accelerator_type,\n",
    "        master_accelerator_count = master_accelerator_count,\n",
    "        parameter_machine_type = parameter_machine_type,\n",
    "        parameter_machine_count = parameter_machine_count,\n",
    "        parameter_image_uri = parameter_image_uri,\n",
    "        parameter_accelerator_type = parameter_accelerator_type,\n",
    "        parameter_accelerator_count = parameter_accelerator_count,\n",
    "        worker_machine_type = worker_machine_type,\n",
    "        worker_machine_count = worker_machine_count,\n",
    "        worker_image_uri = worker_image_uri,\n",
    "        worker_accelerator_type = worker_accelerator_type,\n",
    "        worker_accelerator_count = worker_accelerator_count,\n",
    "        use_chief_in_tf_config = use_chief_in_tf_config,\n",
    "        distribution_strategy_type = distribution_strategy_type)\n",
    "    args.extend(common_args)\n",
    "    print(common_args)\n",
    "    print(master_image_uri)\n",
    "    \n",
    "    args.append('--')\n",
    "    \n",
    "    args.extend(get_run_python_args(func, **kwargs))\n",
    "    args.extend(_get_ai_platform_run_args(job_dir, distribution_strategy_type, use_distribution_strategy_scope))\n",
    "    \n",
    "    print('running training job using Docker image Google Cloud Platform AI:')\n",
    "    print(' '.join(args).replace(' --', '\\n --').replace('\\n', ' \\\\ \\n'))\n",
    "    if not dry_run:\n",
    "        return run_process(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200325_163837'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "echo \"Submitting an AI Platform job...\"\n",
    "# see https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "gcloud ai-platform jobs submit training ${JOB_NAME} \\\n",
    "        --config=${PWD}/scripts/config_fix.yaml \\\n",
    "        --scale-tier=${SCALE_TIER} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=${RUNTIME_VERSION} \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.trainer \\\n",
    "        --package-path=${PACKAGE_PATH}  \\\n",
    "        --master-machine-type=n1-highcpu-16 \\\n",
    "        --stream-logs \\\n",
    "        ${CONFIG} \\\n",
    "        -- \\\n",
    "        --job-dir=${MODEL_DIR} --train-location=cloud $@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from gcp_runner.core import get_run_args, run_process, get_package_name, get_module_name, format_job_dir, print_tensorboard_command\n",
    "from gcp_runner import ai_platform_constants\n",
    "\n",
    "#IMPORTANT: we'll need to either copy entry_point.py to package,\n",
    "# or tell user to declare it and call into similar entry_point logic manually.\n",
    "\n",
    "def run_package(\n",
    "    func,\n",
    "    job_dir,\n",
    "    job_name=None,\n",
    "    runtime_version='2.1', \n",
    "    python_version='3.7', \n",
    "    dry_run=False,\n",
    "    region=None,\n",
    "    scale_tier: ai_platform_constants.ScaleTier =  ai_platform_constants.ScaleTier.CUSTOM,\n",
    "    master_machine_type: ai_platform_constants.MachineType = None,\n",
    "    master_image_uri=None,\n",
    "    master_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    master_accelerator_count=None,\n",
    "    parameter_machine_type: ai_platform_constants.MachineType = None,\n",
    "    parameter_machine_count=None,\n",
    "    parameter_image_uri=None,\n",
    "    parameter_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    parameter_accelerator_count=None,\n",
    "    worker_machine_type: ai_platform_constants.MachineType = None,\n",
    "    worker_machine_count=None,\n",
    "    worker_image_uri=None,\n",
    "    worker_accelerator_type: ai_platform_constants.AcceleratorType = None,\n",
    "    worker_accelerator_count=None,\n",
    "    use_chief_in_tf_config=True,\n",
    "    distribution_strategy_type: ai_platform_constants.DistributionStrategyType = None,\n",
    "    use_distribution_strategy_scope: bool = None,\n",
    "    **kwargs):\n",
    "    \n",
    "    date_time = datetime.datetime.now()\n",
    "    if not job_name:\n",
    "        job_name = 'ai_platform_runner_train_package_' + date_time.strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "    job_dir = format_job_dir(job_dir, date_time=date_time)\n",
    "    print_tensorboard_command(job_dir)        \n",
    "    \n",
    "    # see https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "    package_name = get_package_name()\n",
    "    module_name = get_module_name()\n",
    "    function_name = func.__name__\n",
    "    args = [\n",
    "        'gcloud', 'ai-platform', 'jobs', 'submit', 'training', job_name,\n",
    "        \"--runtime-version=%s\" % runtime_version,\n",
    "        \"--python-version=%s\" % python_version,\n",
    "        \"--stream-logs\",\n",
    "        \"--module-name=gcp_runner.entry_point\",\n",
    "        \"--package-path=%s/%s\" % (os.getcwd(), package_name)]\n",
    "\n",
    "#        \"--module-name=%s.entry_point\" % package_name,\n",
    "    \n",
    "    common_args = _get_common_args(\n",
    "        job_dir,\n",
    "        region = region,\n",
    "        scale_tier = scale_tier,\n",
    "        master_machine_type = master_machine_type,\n",
    "        master_image_uri = master_image_uri,\n",
    "        master_accelerator_type = master_accelerator_type,\n",
    "        master_accelerator_count = master_accelerator_count,\n",
    "        parameter_machine_type = parameter_machine_type,\n",
    "        parameter_machine_count = parameter_machine_count,\n",
    "        parameter_image_uri = parameter_image_uri,\n",
    "        parameter_accelerator_type = parameter_accelerator_type,\n",
    "        parameter_accelerator_count = parameter_accelerator_count,\n",
    "        worker_machine_type = worker_machine_type,\n",
    "        worker_machine_count = worker_machine_count,\n",
    "        worker_image_uri = worker_image_uri,\n",
    "        worker_accelerator_type = worker_accelerator_type,\n",
    "        worker_accelerator_count = worker_accelerator_count,\n",
    "        use_chief_in_tf_config = use_chief_in_tf_config,\n",
    "        distribution_strategy_type = distribution_strategy_type)\n",
    "    args.extend(common_args)\n",
    "    args.append(\"--\")\n",
    "    args.extend(get_run_args(func, **kwargs))\n",
    "    args.extend(_get_ai_platform_run_args(job_dir, distribution_strategy_type, use_distribution_strategy_scope))\n",
    "    \n",
    "    print('running training job using package on Google Cloud Platform AI:')\n",
    "    print(' '.join(args).replace(' --', '\\n --').replace('\\n', ' \\\\ \\n'))\n",
    "    \n",
    "    if not dry_run:\n",
    "        return run_process(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted ai_platform_constants.ipynb.\n",
      "Converted ai_platform_runner.ipynb.\n",
      "Converted entry_point.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted kubernetes_runner.ipynb.\n",
      "Converted local_runner.ipynb.\n",
      "Converted sample_code_test.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from gcp_runner import core\n",
    "core.export_and_reload_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
